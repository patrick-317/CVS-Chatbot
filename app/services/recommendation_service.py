import os
import re
import json
from difflib import SequenceMatcher
from typing import List, Dict, Tuple, Optional

import numpy as np
import pandas as pd
from openai import OpenAI


# ============================================================
# ì „ì—­ ìºì‹œ / ìƒìˆ˜
# ============================================================

_combo_docs: Optional[List[dict]] = None
_combo_embeddings: Optional[np.ndarray] = None
_openai_client: Optional[OpenAI] = None

EMBEDDING_MODEL = "text-embedding-3-small"
DATA_DIR = "data"
PRECOMP_DIR = "precomputed"


# ============================================================
# ê³µí†µ ìœ í‹¸
# ============================================================

def _json_default(o):
    """
    json.dump ì—ì„œ numpy íƒ€ì… ë“±ì„ íŒŒì´ì¬ ê¸°ë³¸ íƒ€ì…ìœ¼ë¡œ ë³€í™˜í•˜ê¸° ìœ„í•œ í—¬í¼
    """
    import numpy as _np

    if isinstance(o, (_np.integer, _np.floating)):
        return o.item()
    return str(o)


def _get_openai_client() -> OpenAI:
    global _openai_client
    if _openai_client is None:
        _openai_client = OpenAI()
    return _openai_client


def _cosine_sim(a: np.ndarray, b: np.ndarray) -> np.ndarray:
    """
    a: (N, D), b: (D,) ë˜ëŠ” (1, D)
    return: (N,) similarity
    """
    if b.ndim == 1:
        b = b[None, :]
    a_norm = a / (np.linalg.norm(a, axis=1, keepdims=True) + 1e-8)
    b_norm = b / (np.linalg.norm(b, axis=1, keepdims=True) + 1e-8)
    return (a_norm @ b_norm.T).reshape(-1)


def _load_csv(path: str) -> pd.DataFrame:
    if not os.path.exists(path):
        raise FileNotFoundError(f"CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path}")
    return pd.read_csv(path)


def _normalize_name(name: str) -> str:
    """
    í¸ì˜ì  ìƒí’ˆëª… ë§¤ì¹­ìš© ì •ê·œí™”:
    - ê³µë°± ì œê±°
    - ì†Œë¬¸ì ë³€í™˜
    - ìˆ«ì/ì˜ë¬¸/í•œê¸€ë§Œ ë‚¨ê¹€
    """
    s = str(name or "")
    s = s.lower()
    s = re.sub(r"\s+", "", s)
    s = re.sub(r"[^0-9a-zê°€-í£]", "", s)
    return s


# ============================================================
# CU ìƒí’ˆ ë§ˆìŠ¤í„° ë¡œë”©
# ============================================================

def _prepare_product_master() -> pd.DataFrame:
    """
    CU ê³µì‹ ìƒí’ˆ ë¦¬ìŠ¤íŠ¸ ë¡œë“œ + ì •ê·œí™” ì»¬ëŸ¼ ì¶”ê°€
    - data/cu_official_products.csv ì‚¬ìš©
    """
    path = os.path.join(DATA_DIR, "cu_official_products.csv")
    df = _load_csv(path)

    # ì´ë¦„ ì»¬ëŸ¼ ì¶”ì¸¡
    if "name" in df.columns:
        name_col = "name"
    else:
        name_candidates = [
            c for c in df.columns
            if any(k in str(c).lower() for k in ["name", "ìƒí’ˆëª…"])
        ]
        if not name_candidates:
            raise ValueError("cu_official_products.csv ì—ì„œ ìƒí’ˆëª… ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        name_col = name_candidates[0]

    # ê°€ê²© ì»¬ëŸ¼ ì¶”ì¸¡
    if "price" in df.columns:
        price_col = "price"
    else:
        price_candidates = [
            c for c in df.columns
            if any(k in str(c).lower() for k in ["price", "ê°€ê²©"])
        ]
        if not price_candidates:
            raise ValueError("cu_official_products.csv ì—ì„œ ê°€ê²© ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        price_col = price_candidates[0]

    df = df[[name_col, price_col]].copy()
    df.rename(columns={name_col: "name", price_col: "price"}, inplace=True)

    def _to_int_price(v) -> Optional[int]:
        if pd.isna(v):
            return None
        s = str(v).replace(",", "").strip()
        s = re.sub(r"[^0-9]", "", s)
        if not s:
            return None
        try:
            return int(s)
        except Exception:
            return None

    df["price"] = df["price"].map(_to_int_price)
    df["name_norm"] = df["name"].map(_normalize_name)

    return df


def _match_item_name(target: str, df_products: pd.DataFrame) -> Tuple[Optional[str], Optional[int]]:
    """
    ê¿€ì¡°í•©ì— ì íŒ ìƒí’ˆëª…(target)ì„ CU ê³µì‹ ìƒí’ˆëª…ì— ë§¤ì¹­
    - ìš°ì„ : ì •ê·œí™” í›„ ì™„ì „ ì¼ì¹˜
    - ê·¸ ë‹¤ìŒ: í¬í•¨ ê´€ê³„
    - ë§ˆì§€ë§‰: fuzzy match (SequenceMatcher, threshold=0.6)
    """
    target_norm = _normalize_name(target)
    if not target_norm:
        return None, None

    prod_names = df_products["name"].tolist()
    prod_norms = df_products["name_norm"].tolist()
    prod_prices = df_products["price"].tolist()

    # 1) exact normalized match
    for name, norm, price in zip(prod_names, prod_norms, prod_prices):
        if norm == target_norm:
            return name, price

    # 2) substring í¬í•¨
    for name, norm, price in zip(prod_names, prod_norms, prod_prices):
        if target_norm in norm or norm in target_norm:
            return name, price

    # 3) fuzzy match
    best_i = None
    best_score = 0.0
    for i, norm in enumerate(prod_norms):
        score = SequenceMatcher(None, target_norm, norm).ratio()
        if score > best_score:
            best_score = score
            best_i = i

    if best_i is not None and best_score >= 0.6:
        return prod_names[best_i], prod_prices[best_i]

    return None, None


# ============================================================
# ì½¤ë³´ CSV â†’ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ìƒì„±
# ============================================================

def _normalize_category(raw: str, default: str = "ê¸°íƒ€") -> str:
    s = str(raw or "").strip()
    if not s:
        return default

    if "ë¼ë©´" in s or "ë¶„ì‹" in s:
        return "ë¼ë©´/ë¶„ì‹"
    if "ê°„í¸" in s or "ì‹ì‚¬" in s or "ë„ì‹œë½" in s:
        return "ì‹ì‚¬ë¥˜"
    if "ë””ì €íŠ¸" in s or "dessert" in s.lower():
        return "ë””ì €íŠ¸"
    if "ì•ˆì£¼" in s or "ì•¼ì‹" in s:
        return "ìˆ ì•ˆì£¼/ì•¼ì‹"

    return default


def _split_items(text: str) -> List[str]:
    """
    'ì½•ì½•ì½• ìŠ¤íŒŒê²Œí‹°, ì˜ì„±ë§ˆëŠ˜í›„ë‘í¬, ëª¨ì§œë ë¼ ì¹˜ì¦ˆ' ê°™ì€ ë¬¸ìì—´ì„
    ëŒ€ì¶© ìƒí’ˆëª… ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê¸° ìœ„í•œ ê°„ë‹¨í•œ ìŠ¤í”Œë¦¬í„°
    """
    if not isinstance(text, str):
        return []
    parts = re.split(r"[,\n]", text)
    items: List[str] = []
    for p in parts:
        name = p.strip()
        if name:
            items.append(name)
    return items


def _build_combo_docs_from_df(
        df: pd.DataFrame,
        df_products: pd.DataFrame,
        id_offset: int,
) -> List[dict]:
    """
    í•˜ë‚˜ì˜ CSV(DataFrame)ì—ì„œ ê¿€ì¡°í•© ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ.
    - 'ì¡°í•© ì´ë¦„', 'ì£¼ìš” ìƒí’ˆ', 'ë³´ì¡° ìƒí’ˆ(ë“¤)', 'ì¹´í…Œê³ ë¦¬', 'í‚¤ì›Œë“œ / ìƒí™©' ì‚¬ìš©
    - ìƒí’ˆëª…ì€ CU ê³µì‹ ìƒí’ˆê³¼ ìµœëŒ€í•œ ë§¤ì¹­í•˜ì—¬ name / price ë¥¼ ì±„ì›€.
    - CUì— ì—†ëŠ” ìƒí’ˆì€ ì œì™¸.
    - ìµœì¢…ì ìœ¼ë¡œ **CU ìƒí’ˆì´ 2ê°œ ì´ìƒì¸ ì¡°í•©ë§Œ** ì‚¬ìš©.
    """
    docs: List[dict] = []

    for ridx, row in df.iterrows():
        combo_name = str(row.get("ì¡°í•© ì´ë¦„", "")).strip()
        if not combo_name:
            continue

        raw_category = row.get("ì¹´í…Œê³ ë¦¬", "")
        category = _normalize_category(raw_category, default="ê¸°íƒ€")

        mood = str(row.get("í‚¤ì›Œë“œ / ìƒí™©", "")).strip()

        main_item = str(row.get("ì£¼ìš” ìƒí’ˆ", "")).strip()
        sub_items = str(row.get("ë³´ì¡° ìƒí’ˆ(ë“¤)", "")).strip()

        all_item_names: List[str] = []
        if main_item:
            all_item_names.append(main_item)
        all_item_names.extend(_split_items(sub_items))

        matched_items: List[dict] = []
        total_price = 0

        for nm in all_item_names:
            official_name, price = _match_item_name(nm, df_products)
            if not official_name:
                continue
            item = {
                "original_name": nm,
                "name": official_name,
                "price": price,
            }
            matched_items.append(item)
            if isinstance(price, int):
                total_price += price

        # CUì— ë§¤ì¹­ëœ ìƒí’ˆì´ 2ê°œ ë¯¸ë§Œì´ë©´ ìŠ¤í‚µ
        if len(matched_items) < 2:
            continue

        item_names_for_text = ", ".join(i["name"] for i in matched_items)

        base_text = (
            f"ê¿€ì¡°í•© ì´ë¦„: {combo_name}. "
            f"ì¹´í…Œê³ ë¦¬: {category}. "
            f"êµ¬ì„± ìƒí’ˆ: {item_names_for_text}. "
        )
        if mood:
            base_text += f"ì–´ìš¸ë¦¬ëŠ” ìƒí™©/ë¶„ìœ„ê¸°: {mood}."

        doc = {
            "id": int(id_offset + ridx),
            "name": combo_name,
            "category": category,
            "items": matched_items,
            "total_price": int(total_price) if total_price > 0 else None,
            "mood": mood,
            "embedding_text": base_text,
        }
        docs.append(doc)

    return docs


def _build_combo_docs() -> List[dict]:
    """
    combination.csv + synthetic_honey_combos_1000.csv ë¥¼ ëª¨ë‘ ì½ì–´ì„œ
    í•˜ë‚˜ì˜ ì½¤ë³´ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜.
    """
    df_products = _prepare_product_master()
    docs: List[dict] = []

    # 1) ì‹¤ì œ ê¿€ì¡°í•© 100ê°œ
    comb_path = os.path.join(DATA_DIR, "combination.csv")
    if os.path.exists(comb_path):
        df_real = _load_csv(comb_path)
        docs.extend(_build_combo_docs_from_df(df_real, df_products, id_offset=0))

    # 2) synthetic ê¿€ì¡°í•© 1000ê°œ
    syn_path = os.path.join(DATA_DIR, "synthetic_honey_combos_1000.csv")
    if os.path.exists(syn_path):
        df_syn = _load_csv(syn_path)
        offset = len(docs)
        docs.extend(_build_combo_docs_from_df(df_syn, df_products, id_offset=offset))

    return docs


# ============================================================
# ì„ë² ë”© ì¸ë±ìŠ¤ (precomputed íŒŒì¼ + ìºì‹œ)
# ============================================================

def _load_semantic_index() -> Tuple[List[dict], np.ndarray]:
    """
    ì„œë²„ ëŸ°íƒ€ì„ì—ì„œ í˜¸ì¶œ:
    - precomputed íŒŒì¼ì´ ìˆìœ¼ë©´ ë¡œë“œ
    - ì—†ìœ¼ë©´ CSVì—ì„œ ì¦‰ì„ ìƒì„± + ì„ë² ë”© ê³„ì‚° í›„ ì €ì¥
    """
    global _combo_docs, _combo_embeddings

    if _combo_docs is not None and _combo_embeddings is not None:
        return _combo_docs, _combo_embeddings

    os.makedirs(PRECOMP_DIR, exist_ok=True)
    docs_path = os.path.join(PRECOMP_DIR, "combo_docs.json")
    emb_path = os.path.join(PRECOMP_DIR, "combo_embeddings.npy")

    # 1) precomputed ê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ë¡œë“œ
    if os.path.exists(docs_path) and os.path.exists(emb_path):
        with open(docs_path, "r", encoding="utf-8") as f:
            _combo_docs = json.load(f)
        _combo_embeddings = np.load(emb_path)
        return _combo_docs, _combo_embeddings

    # 2) ì—†ìœ¼ë©´ CSVì—ì„œ ì¦‰ì„ ìƒì„±
    print("[_load_semantic_index] precomputed íŒŒì¼ì´ ì—†ì–´, CSVì—ì„œ ì¦‰ì„ ìƒì„±í•©ë‹ˆë‹¤.")
    docs = _build_combo_docs()
    if not docs:
        raise RuntimeError("ì½¤ë³´ ë°ì´í„°ë¥¼ í•˜ë‚˜ë„ ë§Œë“¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. CSV êµ¬ì¡°ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

    client = _get_openai_client()
    texts = [d["embedding_text"] for d in docs]

    resp = client.embeddings.create(
        model=EMBEDDING_MODEL,
        input=texts,
    )
    embeds = np.array([d.embedding for d in resp.data], dtype=np.float32)

    _combo_docs = docs
    _combo_embeddings = embeds

    with open(docs_path, "w", encoding="utf-8") as f:
        json.dump(docs, f, ensure_ascii=False, indent=2, default=_json_default)
    np.save(emb_path, embeds)

    return _combo_docs, _combo_embeddings


# ============================================================
# ì¹´í…Œê³ ë¦¬ ì¶”ë¡  (í‚¤ì›Œë“œ + ë‹¤ì´ì–´íŠ¸ ê·œì¹™)
# ============================================================

CATEGORY_KEYWORDS: Dict[str, List[str]] = {
    "ë¼ë©´/ë¶„ì‹": ["ë¼ë©´", "ì»µë¼ë©´", "êµ­ë¬¼ë¼ë©´", "ë–¡ë³¶ì´", "ë¶„ì‹", "ìš°ë™", "íŠ€ê¹€", "ì–´ë¬µ"],
    "ì‹ì‚¬ë¥˜": ["ë°¥", "ì‹ì‚¬", "ë„ì‹œë½", "ê¹€ì¹˜ì°Œê°œ", "ë®ë°¥", "ì¹´ë ˆ", "ì£½", "íŒŒìŠ¤íƒ€", "ë³¶ìŒë°¥"],
    "ê°„í¸ì‹": ["ì‚¼ê°ê¹€ë°¥", "ì£¼ë¨¹ë°¥", "í–„ë²„ê±°", "ìƒŒë“œìœ„ì¹˜", "í•«ë„ê·¸", "í† ìŠ¤íŠ¸"],
    "ë””ì €íŠ¸": [
        "ë””ì €íŠ¸", "ë¹µ", "ì¼€ì´í¬", "ì¿ í‚¤", "ì´ˆì½œë¦¿",
        "ì ¤ë¦¬", "ì•„ì´ìŠ¤í¬ë¦¼", "ë¹™ìˆ˜", "ë‹¬ë‹¬", "ë‹¬ì½¤", "ë‹¬ë‹¤",
    ],
    "ìˆ ì•ˆì£¼/ì•¼ì‹": [
        "ë§¥ì£¼", "ì†Œì£¼", "ì™€ì¸", "ì•ˆì£¼", "ì•¼ì‹",
        "ì¹˜í‚¨", "ì¡±ë°œ", "í¬ì°¨", "í¸ë§¥", "í¸ì˜ì ë§¥ì£¼",
    ],
}


def infer_category_from_text(text: str) -> Optional[str]:
    """
    ìœ ì € ìì—°ì–´ ë¬¸ì¥ì—ì„œ ëŒ€ëµì ì¸ ì¹´í…Œê³ ë¦¬ ì¶”ë¡ 
    - ë‹¤ì´ì–´íŠ¸/ë“ ë“  í‚¤ì›Œë“œë¥¼ ìš°ì„  ì²˜ë¦¬
    """
    text = (text or "").lower()

    # ê·œì¹™ 1: ë‹¤ì´ì–´íŠ¸ ê´€ë ¨ â†’ ì‹ì‚¬ë¥˜
    if any(kw in text for kw in ["ë‹¤ì´ì–´íŠ¸", "ì¹¼ë¡œë¦¬", "ì‚´ì°”", "ì‚´ ì•ˆ", "ì²´ì¤‘", "ìš´ë™ í›„"]):
        return "ì‹ì‚¬ë¥˜"

    # ê·œì¹™ 2: ë“ ë“ /ë°°ê³ íŒŒ/ì¶œì¶œ â†’ ì‹ì‚¬ë¥˜
    if any(kw in text for kw in ["ë“ ë“ ", "ë°°ê³ íŒŒ", "ë°°ê³ í”ˆ", "ì¶œì¶œ"]):
        return "ì‹ì‚¬ë¥˜"

    # ê¸°ë³¸ í‚¤ì›Œë“œ ë§¤ì¹­
    best_cat: Optional[str] = None
    best_score = 0
    for cat, kws in CATEGORY_KEYWORDS.items():
        score = sum(1 for kw in kws if kw in text)
        if score > best_score:
            best_score = score
            best_cat = cat

    return best_cat if best_score > 0 else None


def _apply_diet_hard_filter(user_text: str, docs: List[dict], indices: List[int]) -> List[int]:
    """
    ìœ ì €ê°€ ë‹¤ì´ì–´íŠ¸ ê´€ë ¨ ë°œí™”ë¥¼ í–ˆì„ ë•Œ,
    ë””ì €íŠ¸/ê³¼ì/ë¹™ìˆ˜ + ë¼ë©´/ì•¼ì‹ ì¹´í…Œê³ ë¦¬ë¥¼ ìµœëŒ€í•œ ì œì™¸.
    """
    text = (user_text or "").lower()
    diet_mode = any(kw in text for kw in ["ë‹¤ì´ì–´íŠ¸", "ì¹¼ë¡œë¦¬", "ì‚´ì°”", "ì‚´ ì•ˆ", "ì²´ì¤‘", "ìš´ë™ í›„"])
    if not diet_mode:
        return indices

    # ğŸ‘‰ ë‹¤ì´ì–´íŠ¸ ëª¨ë“œì—ì„œ í”¼í•˜ê³  ì‹¶ì€ ì¹´í…Œê³ ë¦¬
    bad_categories = ["ë¼ë©´/ë¶„ì‹", "ìˆ ì•ˆì£¼/ì•¼ì‹", "ë””ì €íŠ¸"]

    # ğŸ‘‰ ë‹¤ì´ì–´íŠ¸ ëª¨ë“œì—ì„œ í”¼í•˜ê³  ì‹¶ì€ ë‹¨ì–´ë“¤ (ê³ ì¹¼ë¡œë¦¬/ì•¼ì‹ ëŠë‚Œ)
    bad_words = [
        "ë¹™ìˆ˜", "ì•„ì´ìŠ¤í¬ë¦¼", "ì¼€ì´í¬", "ì¿ í‚¤", "ì´ˆì½œë¦¿", "ì´ˆì½”",
        "ì ¤ë¦¬", "ë‹¬ë‹¬", "ë‹¬ì½¤", "ë””ì €íŠ¸",
        "ë¼ë©´", "ë§¤ìš´", "ë§¤ì½¤", "ì¹˜í‚¨", "ì•¼ì‹", "ë§¥ì£¼", "ì†Œì£¼",
    ]

    filtered: List[int] = []
    for i in indices:
        d = docs[i]

        # 1) ì¹´í…Œê³ ë¦¬ë¡œ ë¨¼ì € ì»·
        cat = str(d.get("category", ""))
        if cat in bad_categories:
            continue

        # 2) í…ìŠ¤íŠ¸ ë‚´ìš©ìœ¼ë¡œ í•œ ë²ˆ ë” ì»·
        content = (
                str(d.get("embedding_text", "")) + " "
                + str(d.get("mood", "")) + " "
                + str(d.get("name", ""))
        )
        if any(bw in content for bw in bad_words):
            continue

        filtered.append(i)

    # ì „ë¶€ ê±¸ëŸ¬ì¡Œìœ¼ë©´ ì›ë˜ ë¦¬ìŠ¤íŠ¸ ìœ ì§€ (ì‘ë‹µì´ ë¹„ëŠ” ê²ƒ ë°©ì§€)
    return filtered if filtered else indices


# ============================================================
# ì¶”ì²œ API (ì¹´ì¹´ì˜¤ ì»¨íŠ¸ë¡¤ëŸ¬ì—ì„œ ì§ì ‘ í˜¸ì¶œ)
# ============================================================

def recommend_combos_openai_rag(
        user_text: str,
        top_k: int = 3,
        min_items: int = 2,
) -> List[dict]:
    """
    - user_text ì„ë² ë”©
    - ì‚¬ì „ ê³„ì‚°ëœ combo_embeddings ì™€ cosine similarity
    - ì¹´í…Œê³ ë¦¬ ë° ìµœì†Œ ìƒí’ˆ ê°œìˆ˜ ì¡°ê±´ì„ ê³ ë ¤í•´ ìƒìœ„ top_k ê°œ ì¡°í•© ë¦¬í„´
    """
    docs, embeds = _load_semantic_index()
    if not docs:
        return []

    # í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¹´í…Œê³ ë¦¬ ì¶”ë¡ 
    inferred_cat = infer_category_from_text(user_text)

    # 1ì°¨ í›„ë³´: ì¹´í…Œê³ ë¦¬ + ìµœì†Œ ìƒí’ˆ ê°œìˆ˜ ì¡°ê±´
    candidate_indices: List[int] = []
    for i, d in enumerate(docs):
        items = d.get("items", [])
        if not isinstance(items, list) or len(items) < min_items:
            continue

        if inferred_cat:
            if d.get("category") == inferred_cat:
                candidate_indices.append(i)
        else:
            candidate_indices.append(i)

    # ì¹´í…Œê³ ë¦¬ ê¸°ì¤€ìœ¼ë¡œ ì•„ë¬´ê²ƒë„ ì—†ìœ¼ë©´, ìµœì†Œ ìƒí’ˆ ì¡°ê±´ë§Œìœ¼ë¡œ ì „ì²´ì—ì„œ ê²€ìƒ‰
    if not candidate_indices:
        candidate_indices = [
            i for i, d in enumerate(docs)
            if isinstance(d.get("items", []), list)
               and len(d.get("items", [])) >= min_items
        ]

    if not candidate_indices:
        return []

    # ë‹¤ì´ì–´íŠ¸ ëª¨ë“œ í•˜ë“œ í•„í„° ì ìš©
    candidate_indices = _apply_diet_hard_filter(user_text, docs, candidate_indices)

    # user_text ì„ë² ë”©
    client = _get_openai_client()
    resp = client.embeddings.create(
        model=EMBEDDING_MODEL,
        input=[user_text],
    )
    q_emb = np.array(resp.data[0].embedding, dtype=np.float32)

    # í›„ë³´ë“¤ì— ëŒ€í•´ì„œë§Œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    cand_embeds = embeds[candidate_indices]
    sims = _cosine_sim(cand_embeds, q_emb)

    # top_k * 3 ì •ë„ ë„‰ë„‰í•˜ê²Œ ë½‘ì€ í›„ í•„í„°ë§
    top_n = min(len(candidate_indices), top_k * 3)
    order = np.argsort(-sims)[:top_n]

    results: List[dict] = []
    for ord_idx in order:
        doc_idx = candidate_indices[int(ord_idx)]
        d = docs[doc_idx]

        items = d.get("items", [])
        if not isinstance(items, list) or len(items) < min_items:
            continue

        results.append(
            {
                "id": d["id"],
                "name": d["name"],
                "category": d.get("category", "ê¸°íƒ€"),
                "items": items,
                "total_price": d.get("total_price"),
                "mood": d.get("mood", ""),
            }
        )

        if len(results) >= top_k:
            break

    return results
