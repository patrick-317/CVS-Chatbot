import os
import re
import json
from typing import List, Dict, Any, Optional

import numpy as np
import pandas as pd
from openai import OpenAI

# =====================================================================
# ê²½ë¡œ ì„¤ì •
# =====================================================================
BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
COMBINATION_PATH = os.path.join(BASE_DIR, "combination.csv")
SYNTHETIC_PATH = os.path.join(BASE_DIR, "synthetic_honey_combos_1000.csv")
CU_PRODUCTS_PATH = os.path.join(BASE_DIR, "cu_official_products.csv")

PRECOMPUTED_DIR = os.path.join(BASE_DIR, "precomputed")
COMBO_EMBEDDINGS_PATH = os.path.join(PRECOMPUTED_DIR, "combo_embeddings.npy")
COMBO_DOCS_PATH = os.path.join(PRECOMPUTED_DIR, "combo_docs.json")

# =====================================================================
# ì „ì—­ ìºì‹œ
# =====================================================================
_comb_df: Optional[pd.DataFrame] = None
_syn_df: Optional[pd.DataFrame] = None
_cu_df: Optional[pd.DataFrame] = None
_combo_df: Optional[pd.DataFrame] = None

_keyword_dict: Optional[Dict[str, set]] = None

_openai_client: Optional[OpenAI] = None
_openai_embedding_model: str = "text-embedding-3-small"

_combo_embeddings: Optional[np.ndarray] = None  # N x d

# =====================================================================
# ğŸ”¹ ì¹´í…Œê³ ë¦¬ ì¶”ë¡ ìš© í‚¤ì›Œë“œ (ì»¨íŠ¸ë¡¤ëŸ¬ì—ì„œ ì‚¬ìš©)
# =====================================================================
CATEGORY_KEYWORDS = {
    "ë¼ë©´/ë¶„ì‹": ["ë¼ë©´", "ì»µë¼ë©´", "êµ­ë¬¼ë¼ë©´", "ë–¡ë³¶ì´", "ë¶„ì‹", "ìš°ë™", "íŠ€ê¹€", "ì–´ë¬µ"],
    "ì‹ì‚¬ë¥˜": ["ë°¥", "ì‹ì‚¬", "ë„ì‹œë½", "ë®ë°¥", "ì¹´ë ˆ", "ì£½", "íŒŒìŠ¤íƒ€", "ë³¶ìŒë°¥"],
    "ê°„í¸ì‹": ["ì‚¼ê°ê¹€ë°¥", "ì£¼ë¨¹ë°¥", "í–„ë²„ê±°", "ìƒŒë“œìœ„ì¹˜", "í•«ë„ê·¸", "í† ìŠ¤íŠ¸"],
    "ë””ì €íŠ¸": ["ë¹µ", "ì¼€ì´í¬", "ì¿ í‚¤", "ì´ˆì½”", "ì ¤ë¦¬", "ì•„ì´ìŠ¤í¬ë¦¼", "ë¹™ìˆ˜", "ë‹¬ë‹¬", "ë‹¬ì½¤"],
    "ìˆ ì•ˆì£¼/ì•¼ì‹": ["ë§¥ì£¼", "ì†Œì£¼", "ì™€ì¸", "ì•ˆì£¼", "ì•¼ì‹", "ì¹˜í‚¨", "í¬ì°¨", "í¸ë§¥"],
}


def infer_category_from_text(text: str) -> str:
    """ì‚¬ìš©ì ë¬¸ì¥ì—ì„œ ëŒ€ëµì ì¸ ì¹´í…Œê³ ë¦¬ ì¶”ë¡  (quickRepliesìš©)"""
    if not text:
        return ""

    low = text.lower()
    best_cat = ""
    best_score = 0

    for cat, kws in CATEGORY_KEYWORDS.items():
        score = 0
        for kw in kws:
            if kw in text or kw in low:
                score += 1
        if score > best_score:
            best_cat = cat
            best_score = score

    return best_cat


# =====================================================================
# OpenAI client
# =====================================================================
def _get_openai_client() -> OpenAI:
    global _openai_client
    if _openai_client is None:
        _openai_client = OpenAI()
    return _openai_client


# =====================================================================
# í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
# =====================================================================
def _clean_text(text: str) -> str:
    if not isinstance(text, str):
        return ""
    t = re.sub(r"\(.*?\)", "", text)
    t = re.sub(r"[^0-9a-zA-Zê°€-í£]", "", t)
    return t.lower()


# =====================================================================
# ë°ì´í„° ë¡œë”©
# =====================================================================
def _load_data():
    global _comb_df, _syn_df, _cu_df, _combo_df, _keyword_dict

    if _combo_df is not None:
        return

    _comb_df = pd.read_csv(COMBINATION_PATH)
    _syn_df = pd.read_csv(SYNTHETIC_PATH)
    _cu_df = pd.read_csv(CU_PRODUCTS_PATH)

    _comb_df["source"] = "real"
    _syn_df["source"] = "synthetic"

    _combo_df = pd.concat([_comb_df, _syn_df], ignore_index=True)

    for col in ["ì¡°í•© ì´ë¦„", "ì£¼ìš” ìƒí’ˆ", "ë³´ì¡° ìƒí’ˆ(ë“¤)", "í‚¤ì›Œë“œ / ìƒí™©", "ì¹´í…Œê³ ë¦¬", "source"]:
        if col in _combo_df.columns:
            _combo_df[col] = _combo_df[col].fillna("")

    if "clean_name" not in _cu_df.columns:
        _cu_df["clean_name"] = _cu_df["name"].apply(_clean_text)

    _keyword_dict = _build_keyword_dict()


def _build_keyword_dict() -> Dict[str, set]:
    global _combo_df
    d: Dict[str, set] = {}
    for val in _combo_df["í‚¤ì›Œë“œ / ìƒí™©"]:
        if not isinstance(val, str):
            continue

        parts = re.split(r"[;,]", val)
        for p in parts:
            p = p.strip()
            if not p:
                continue

            low = p.lower()
            compact = low.replace(" ", "")
            for k in {low, compact}:
                d.setdefault(k, set()).add(p)
    return d


# =====================================================================
# RAG ê¸°ë°˜: ê° ì¡°í•©ì´ ì™œ ì¢‹ì€ì§€ í•œ ì¤„ ì„¤ëª… ìƒì„± (ì˜¤í”„ë¼ì¸ìš©)
# =====================================================================
def _rag_extract_combo_features(row: pd.Series) -> str:
    prompt = f"""
    ì•„ë˜ í¸ì˜ì  ê¿€ì¡°í•©ì´ ì™œ ì¢‹ì€ ì¡°í•©ì¸ì§€ ì„¤ëª…í•´ì¤˜.
    'ë§› ì¡°í™”(ë§¤ìš´/ë‹¨/ì§ /ê³ ì†Œ)', 'ì‹ê° ëŒ€ë¹„(ë°”ì‚­/ì«€ë“/ë¶€ë“œëŸ¬ì›€)', 
    'ì˜¨ë„ ëŒ€ë¹„(ëœ¨ê±°ì›€+ì°¨ê°€ì›€)', 'ì¤‘í™”/ë°¸ëŸ°ìŠ¤(ë§¤ìš´ë§›+ì¹˜ì¦ˆ, ì§ ë§›+ë‹¨ë§›)', 
    'í¬ë§Œê°', 'ìƒí™©(ì•¼ì‹/ë‹¤ì´ì–´íŠ¸/ê°„í¸ì‹)' ê´€ì ì—ì„œ 1~2ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì¤˜.

    ì£¼ìš”ìƒí’ˆ: {row['ì£¼ìš” ìƒí’ˆ']}
    ë³´ì¡°ìƒí’ˆ: {row['ë³´ì¡° ìƒí’ˆ(ë“¤)']}
    ìƒí™©/í‚¤ì›Œë“œ: {row['í‚¤ì›Œë“œ / ìƒí™©']}
    """
    resp = _get_openai_client().chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.4,
    )
    return resp.choices[0].message.content.strip()


# =====================================================================
# Bì•ˆ í•µì‹¬: ì„ë² ë”©ì„ ë¯¸ë¦¬ ê³„ì‚°í•´ì„œ íŒŒì¼ë¡œ ì €ì¥ (í•œ ë²ˆë§Œ ì‹¤í–‰)
# =====================================================================
def build_precomputed_embeddings():
    _load_data()

    os.makedirs(PRECOMPUTED_DIR, exist_ok=True)

    docs: List[str] = []
    print(f"[build_precomputed_embeddings] ì´ {_combo_df.shape[0]} ê°œ ì¡°í•© ì²˜ë¦¬ ì¤‘...")

    for _, row in _combo_df.iterrows():
        try:
            reason = _rag_extract_combo_features(row)
        except Exception:
            # ì‹¤íŒ¨ ì‹œ ì´ìœ  ì—†ì´ë„ ì§„í–‰
            reason = ""

        doc = " / ".join(
            [
                f"ì¡°í•© ì´ë¦„: {row['ì¡°í•© ì´ë¦„']}",
                f"ì£¼ìš” ìƒí’ˆ: {row['ì£¼ìš” ìƒí’ˆ']}",
                f"ë³´ì¡° ìƒí’ˆ: {row['ë³´ì¡° ìƒí’ˆ(ë“¤)']}",
                f"ìƒí™©: {row['í‚¤ì›Œë“œ / ìƒí™©']}",
                f"ì¹´í…Œê³ ë¦¬: {row['ì¹´í…Œê³ ë¦¬']}",
                f"ì´ìœ : {reason}",
            ]
        )
        docs.append(doc)

    # ì„ë² ë”© ê³„ì‚°
    client = _get_openai_client()
    embeddings: List[List[float]] = []
    batch_size = 100

    for i in range(0, len(docs), batch_size):
        chunk = docs[i : i + batch_size]
        resp = client.embeddings.create(
            model=_openai_embedding_model,
            input=chunk,
        )
        for d in resp.data:
            embeddings.append(d.embedding)

    arr = np.array(embeddings, dtype="float32")
    norms = np.linalg.norm(arr, axis=1, keepdims=True) + 1e-10
    arr = arr / norms

    np.save(COMBO_EMBEDDINGS_PATH, arr)

    with open(COMBO_DOCS_PATH, "w", encoding="utf-8") as f:
        json.dump(docs, f, ensure_ascii=False, indent=2)

    print(f"[build_precomputed_embeddings] ì €ì¥ ì™„ë£Œ: {COMBO_EMBEDDINGS_PATH}")


# =====================================================================
# ì„œë²„ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì„ë² ë”© ë¡œë” (ë¹ ë¥¸ ê²½ë¡œ)
# =====================================================================
def _load_semantic_index():
    global _combo_embeddings
    _load_data()

    if _combo_embeddings is not None:
        return

    if os.path.exists(COMBO_EMBEDDINGS_PATH):
        arr = np.load(COMBO_EMBEDDINGS_PATH)
        _combo_embeddings = arr.astype("float32")
        return

    # ğŸ”» fallback: RAG ì´ìœ  ì—†ì´ ê°„ë‹¨ í…ìŠ¤íŠ¸ë¡œ ì„ë² ë”© ìƒì„± (ìµœì´ˆ 1íšŒ)
    client = _get_openai_client()
    docs: List[str] = []
    for _, row in _combo_df.iterrows():
        doc = " / ".join(
            [
                f"ì¡°í•© ì´ë¦„: {row['ì¡°í•© ì´ë¦„']}",
                f"ì£¼ìš” ìƒí’ˆ: {row['ì£¼ìš” ìƒí’ˆ']}",
                f"ë³´ì¡° ìƒí’ˆ: {row['ë³´ì¡° ìƒí’ˆ(ë“¤)']}",
                f"ìƒí™©: {row['í‚¤ì›Œë“œ / ìƒí™©']}",
                f"ì¹´í…Œê³ ë¦¬: {row['ì¹´í…Œê³ ë¦¬']}",
            ]
        )
        docs.append(doc)

    embeddings: List[List[float]] = []
    batch_size = 100
    for i in range(0, len(docs), batch_size):
        chunk = docs[i : i + batch_size]
        resp = client.embeddings.create(
            model=_openai_embedding_model,
            input=chunk,
        )
        for d in resp.data:
            embeddings.append(d.embedding)

    arr = np.array(embeddings, dtype="float32")
    norms = np.linalg.norm(arr, axis=1, keepdims=True) + 1e-10
    arr = arr / norms

    _combo_embeddings = arr
    # ì›í•˜ë©´ ì—¬ê¸°ì„œë„ npyë¡œ ì €ì¥ ê°€ëŠ¥
    os.makedirs(PRECOMPUTED_DIR, exist_ok=True)
    np.save(COMBO_EMBEDDINGS_PATH, arr)


# =====================================================================
# í‚¤ì›Œë“œ ì¶”ì¶œ
# =====================================================================
def extract_keywords(text: str) -> List[str]:
    raw = text.lower()
    compact = re.sub(r"[^0-9a-zA-Zê°€-í£]", "", raw)

    found = set()
    for trig, concept_set in _keyword_dict.items():
        if trig in compact:
            found |= concept_set

    if not found:
        parts = re.split(r"\s+|[,./!?]", raw)
        found = {p for p in parts if len(p) >= 2}

    return list(found)


# =====================================================================
# CU ìƒí’ˆ ë§¤ì¹­ (ë¹„ì‹í’ˆ í•„í„°)
# =====================================================================
def _is_food_product(row: pd.Series) -> bool:
    name = str(row.get("name", "")).lower()
    non_food = ["ìš°ì‚°", "ì´ì–´í°", "ì¶©ì „", "usb", "ë¼ì´í„°", "ë¬¼í‹°ìŠˆ", "ê±´ì „ì§€"]
    for n in non_food:
        if n in name:
            return False
    return True


def _find_cu_products(row: pd.Series, max_items: int = 3) -> List[str]:
    global _cu_df
    _load_data()

    combo_items = f"{row['ì£¼ìš” ìƒí’ˆ']},{row['ë³´ì¡° ìƒí’ˆ(ë“¤)']}"
    parts = re.split(r"[,+/Â·]|ì™¸", combo_items)

    results: List[str] = []

    for item in parts:
        item = item.strip()
        if not item:
            continue

        clean = _clean_text(item)
        best = None
        best_score = 0

        for _, cu in _cu_df.iterrows():
            cu_clean = cu["clean_name"]
            score = 0
            if clean and clean in cu_clean:
                score = len(clean)
            elif cu_clean and cu_clean in clean:
                score = len(cu_clean)

            if score > best_score:
                best_score = score
                best = cu["name"]

        if not best:
            continue

        cu_row = _cu_df[_cu_df["name"] == best]
        if cu_row.empty or not _is_food_product(cu_row.iloc[0]):
            continue

        if best not in results:
            results.append(best)

        if len(results) >= max_items:
            break

    return results


# =====================================================================
# ğŸ”¥ ìµœì¢… ì¶”ì²œ í•¨ìˆ˜ â€” ì»¨íŠ¸ë¡¤ëŸ¬ì—ì„œ í˜¸ì¶œ
# =====================================================================
def recommend_combos_openai_rag(user_text: str, top_k: int = 3) -> List[Dict[str, Any]]:
    global _combo_embeddings, _combo_df

    _load_data()
    _load_semantic_index()

    if not user_text:
        user_text = "ì•„ë¬´ê±°ë‚˜ ì¶”ì²œí•´ì¤˜"

    client = _get_openai_client()

    # 1) ì‚¬ìš©ì ë¬¸ì¥ ì„ë² ë”© (í•œ ë²ˆ)
    resp = client.embeddings.create(
        model=_openai_embedding_model,
        input=[user_text],
    )
    q = np.array(resp.data[0].embedding, dtype="float32")
    q = q / (np.linalg.norm(q) + 1e-10)

    # 2) ì½”ì‚¬ì¸ ìœ ì‚¬ë„
    sims = _combo_embeddings @ q

    # 3) í‚¤ì›Œë“œ ê¸°ë°˜ ìŠ¤ì½”ì–´
    keywords = extract_keywords(user_text)
    kw_scores = []
    for _, row in _combo_df.iterrows():
        txt = " ".join(
            [
                row["ì¡°í•© ì´ë¦„"],
                row["ì£¼ìš” ìƒí’ˆ"],
                row["ë³´ì¡° ìƒí’ˆ(ë“¤)"],
                row["í‚¤ì›Œë“œ / ìƒí™©"],
                row["ì¹´í…Œê³ ë¦¬"],
            ]
        ).lower()
        score = sum(1 for kw in keywords if kw.lower() in txt)
        kw_scores.append(score)

    kw_scores = np.array(kw_scores, dtype="float32")
    if kw_scores.max() > 0:
        kw_scores /= (kw_scores.max() + 1e-10)

    # 4) ì‹¤ì œ ê¿€ì¡°í•©(real) ë³´ë„ˆìŠ¤
    is_real = (_combo_df["source"] == "real").astype("float32").to_numpy()

    # 5) ìµœì¢… ì ìˆ˜
    final = 0.75 * sims + 0.20 * kw_scores + 0.05 * is_real

    ordered = list(np.argsort(-final))

    kw_preview = ", ".join(keywords[:3]) if keywords else user_text[:20]

    results: List[Dict[str, Any]] = []

    for idx in ordered:
        row = _combo_df.iloc[idx]
        items = _find_cu_products(row, max_items=3)
        if len(items) < 2:
            continue

        reason = (
            "ì…ë ¥í•˜ì‹  ë¬¸ì¥ì˜ ì˜ë¯¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë¶„ì„í•´ì„œ "
            f"ê°€ì¥ ë¹„ìŠ·í•œ ë¶„ìœ„ê¸°ì˜ ê¿€ì¡°í•©ì„ ê³¨ëì–´ìš”. (ê¸°ì¤€: '{kw_preview}')"
        )
        if row["source"] == "real":
            reason += "\nì‹¤ì œë¡œ ë§ì´ ì•Œë ¤ì§„ ê¿€ì¡°í•©ì´ë¼ì„œ ìš°ì„ ì ìœ¼ë¡œ ì¶”ì²œí–ˆì–´ìš”."

        results.append(
            {
                "name": row["ì¡°í•© ì´ë¦„"],
                "category": row["ì¹´í…Œê³ ë¦¬"],
                "reason": reason,
                "items": items,
            }
        )

        if len(results) >= top_k:
            break

    return results
